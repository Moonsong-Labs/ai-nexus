{
  "inputs": {
    "repository": "Moonsong-Labs/ai-nexus",
    "pr_num": "49"
  },
  "outputs": {
    "output": "diff --git a/project_memories/global.md b/project_memories/global.md\nindex 18fd2cc..46b1c8b 100644\n--- a/project_memories/global.md\n+++ b/project_memories/global.md\n@@ -7,7 +7,7 @@\n **Core Mission:** To develop a system for managing and orchestrating a team of AI agents capable of designing, developing, and maintaining technical projects. An initial focus is on an agent named \"Cursor\" which operates with a memory that resets between sessions, necessitating a robust external \"Memory Bank\" system for continuity. AI Nexus aims to be a platform for developing and managing such AI agents.\n \n **Key Concepts:**\n-1.  **Multi-Agent System:** The project involves a team of specialized AI agents (Orchestrator, Architect, Coder, Tester, Code Reviewer, Requirement Gatherer, Grumpy) working collaboratively.\n+1.  **Multi-Agent System:** The project involves a team of specialized AI agents (Orchestrator, Architect, Coder, Tester, Code Reviewer, Requirement Gatherer, Grumpy, Task Manager) working collaboratively.\n 2.  **Externalized Memory (Memory Bank):** A core principle, especially for the \"Cursor\" concept, where agents rely on structured external files (primarily Markdown) for persistent knowledge, project state, and context. This addresses context loss in AI agents.\n 3.  **LangGraph Framework:** The primary framework used for building the AI agents, defining their state, and managing their execution flow.\n 4.  **Tool-Using Agents:** Agents are equipped with tools to perform actions, interact with systems (like GitHub), and manage their memory.\n@@ -163,7 +163,7 @@ This file outlines the overarching standards and technological choices for the A\n     *   **openevals:** Suggests involvement in evaluating language models.\n *   **Version Control:** Git.\n *   **LLM Models:**\n-    *   **`gemini-1.5-flash-latest` (or similar flash variants):** Preferred for simple tasks, quick evaluations. (PRD mentions `gemini-2.0-flash`, current common models are 1.5 series. The intent is a fast model.)\n+    *   **`gemini-1.5-flash-latest` (or similar flash variants like `gemini-2.5-flash-preview-04-17`):** Preferred for simple tasks, quick evaluations. (PRD mentions `gemini-2.0-flash`, current common models are 1.5 series. The intent is a fast model.)\n     *   **`gemini-1.5-pro-latest` (or similar pro variants):** Preferred for complex tasks needing reasoning. (PRD mentions `gemini-2.5-pro-preview-03-25`, intent is a powerful model.)\n \n \n@@ -595,6 +595,23 @@ Most agents in AI Nexus follow a common structural and operational pattern, larg\n *   **`tools.py` (`src/grumpy/tools.py`):** Standard `upsert_memory` tool.\n *   **`utils.py` (`src/grumpy/utils.py`):** Standard utilities.\n \n+#### 5.8. Task Manager (`src/task_manager/`)\n+\n+*   **Role:** Analyzes requests and determines its own input document requirements for task planning. Based on current tests, it identifies `prd.md`, `techstack.md`, and `split_criteria.md` as necessary documents for its operation.\n+*   **Structure:** Follows the `agent_template` pattern.\n+    *   `configuration.py`:\n+        *   `TASK_MANAGER_MODEL = \"google_genai:gemini-2.5-flash-preview-04-17\"`.\n+        *   Standard `Configuration` dataclass using this model and `prompts.SYSTEM_PROMPT`.\n+    *   `graph.py`:\n+        *   Standard `agent_template` flow: `call_model`, `store_memory`, `route_message`.\n+        *   LLM initialized with `TASK_MANAGER_MODEL`.\n+        *   `call_model` node includes logic for static and dynamic memories and binds the `upsert_memory` tool.\n+        *   The graph builder (`builder`) is exported along with the compiled graph.\n+    *   `prompts.py`: Assumed to contain `SYSTEM_PROMPT` (details not in PR).\n+    *   `state.py`: Standard `State` with `messages`.\n+    *   `tools.py`: Standard `upsert_memory` tool.\n+    *   `utils.py`: Standard utilities.\n+\n \n ## 6. Testing Framework (`tests/`)\n \n@@ -620,6 +637,13 @@ The project uses `pytest` for testing and integrates with LangSmith for evaluati\n         *   Creates a LangSmith dataset using `client.create_dataset()`.\n         *   Adds examples (input-output pairs) to the dataset using `client.create_examples()`. Inputs are simple strings, outputs are expected agent responses.\n \n+*   **`tests/datasets/task_manager_dataset.py`:**\n+    *   Defines `TASK_MANAGER_DATASET_NAME = \"task-manager-requirements\"`.\n+    *   `create_dataset()` function:\n+        *   Initializes `Client()`.\n+        *   Creates a LangSmith dataset.\n+        *   Adds an example where the input is \"What requirements do you need to make your work done?\" and the expected output is \"In order to create a task planning, I need to have access to prd.md, techstack.md and split_criteria.md files.\"\n+\n *   **`tests/integration_tests/`:**\n     *   **`test_graph.py`:**\n         *   `test_memory_storage`: Basic test for the `agent_template` graph's memory storage.\n@@ -642,6 +666,12 @@ The project uses `pytest` for testing and integrates with LangSmith for evaluati\n         *   Tests the grumpy agent against a LangSmith dataset (e.g., `LANGSMITH_DATASET_NAME = \"grumpy-failed-questions\"`).\n         *   Uses `LLMJudge` from `tests.testing.evaluators` to create a `correctness_evaluator` with a specific prompt for judging Grumpy's output.\n         *   The `create_graph_caller` utility is used to wrap the Grumpy agent's graph for evaluation.\n+    *   **`test_task_manager.py`:**\n+        *   Tests the Task Manager agent against `TASK_MANAGER_DATASET_NAME`.\n+        *   Uses `LLMJudge().create_correctness_evaluator(plaintext=True)`.\n+        *   `create_task_manager_graph_caller`: A specialized wrapper to invoke the Task Manager graph, handling input formatting (from dataset structure to `HumanMessage`) and setting a specific `RunnableConfig` (including `model: \"google_genai:gemini-2.5-flash-preview-04-17\"`).\n+        *   The Task Manager graph builder is compiled with `MemorySaver()`.\n+        *   `client.aevaluate()` is used with `num_repetitions=4` and `max_concurrency=4`.\n \n *   **`tests/testing/__init__.py`:**\n     *   `get_logger()`: Utility to create a Python logger with a default format.\n@@ -658,6 +688,7 @@ The project uses `pytest` for testing and integrates with LangSmith for evaluati\n         *   `create_llm_as_judge(prompt: str, input_keys: List[str], output_key: str, reference_output_key: str, continuous: bool = True)`:\n             *   Creates an evaluator chain using an LLM.\n             *   Takes a prompt template, keys for input, output, reference, and a flag for continuous feedback.\n+        *   `create_correctness_evaluator(plaintext: bool = False)`: Method added to `LLMJudge` (implied by usage in `test_task_manager.py`) to create a correctness evaluator, potentially with a plaintext mode.\n     *   `CORRECTNESS_PROMPT`: A prompt template for an LLM to judge if the `prediction` matches the `reference` output given an `input`.\n     *   `correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict)`:\n         *   A specific evaluator instance created using `LLMJudge().create_llm_as_judge` with `CORRECTNESS_PROMPT`.\n@@ -684,6 +715,11 @@ The project uses `pytest` for testing and integrates with LangSmith for evaluati\n     *   `make test_unit`: Runs unit tests (`uv run -- pytest tests/unit_tests`).\n     *   `make test_integration`: Runs integration tests (`uv run -- pytest tests/integration_tests`).\n     *   `make test`: Runs both `test_unit` and `test_integration`.\n+    *   `make test-architect`: Runs Architect agent specific tests.\n+    *   `make test-task-manager`: Runs Task Manager agent integration tests (`uv run -- pytest -rs tests/integration_tests/test_task_manager.py`).\n+    *   `make test_watch`: Runs unit tests with `pytest-watch` for automatic re-runs on file changes.\n+    *   `make set-requirement-dataset`: Creates/updates the LangSmith dataset for the Requirement Gatherer.\n+    *   `make set-task-manager-dataset`: Creates/updates the LangSmith dataset for the Task Manager (`uv run --env-file .env -- python tests/datasets/task_manager_dataset.py`).\n *   **Configuration:** `.env` file (copied from `.env.example`) for environment variables.\n     *   Required for Google AI services: `GOOGLE_API_KEY` (this is the preferred variable). Alternatively, `GEMINI_API_KEY` can be set; scripts will use `GOOGLE_API_KEY` if present, otherwise they will use `GEMINI_API_KEY`.\n     *   Optional for Coder agent: `GITHUB_APP_ID`, `GITHUB_APP_PRIVATE_KEY`, `GITHUB_REPOSITORY`.\n@@ -752,17 +788,27 @@ ai-nexus/\n \u2502   \u2502   \u251c\u2500\u2500 memory/               # Markdown files defining Orchestrator's rules and team\n \u2502   \u2502   \u2514\u2500\u2500 stubs/                # Stub implementations for delegated agent calls (for testing/dev)\n \u2502   \u251c\u2500\u2500 requirement_gatherer/     # Requirement Gatherer agent: elicits and clarifies requirements\n+\u2502   \u251c\u2500\u2500 task_manager/             # Task Manager agent: identifies its input document needs\n+\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n+\u2502   \u2502   \u251c\u2500\u2500 configuration.py\n+\u2502   \u2502   \u251c\u2500\u2500 graph.py\n+\u2502   \u2502   \u251c\u2500\u2500 prompts.py            # (Assumed)\n+\u2502   \u2502   \u251c\u2500\u2500 state.py              # (Assumed, standard)\n+\u2502   \u2502   \u251c\u2500\u2500 tools.py              # (Assumed, standard upsert_memory)\n+\u2502   \u2502   \u2514\u2500\u2500 utils.py              # (Assumed, standard)\n \u2502   \u2514\u2500\u2500 tester/                   # Tester agent: generates tests based on requirements\n \u2502       \u251c\u2500\u2500 output.py             # Pydantic models for Tester's structured output\n \u2502       \u251c\u2500\u2500 test-agent-system-prompt.md # Detailed system prompt for Tester\n \u2502       \u2514\u2500\u2500 test-prompts/         # Example requirements for Tester (e.g., web-api.md)\n \u2514\u2500\u2500 tests/                        # Automated tests\n     \u251c\u2500\u2500 datasets/                 # Scripts for creating LangSmith datasets\n-    \u2502   \u2514\u2500\u2500 requirement_gatherer_dataset.py\n+    \u2502   \u251c\u2500\u2500 requirement_gatherer_dataset.py\n+    \u2502   \u2514\u2500\u2500 task_manager_dataset.py # Dataset for Task Manager\n     \u251c\u2500\u2500 integration_tests/        # Integration tests for agents and full graph functionality\n     \u2502   \u251c\u2500\u2500 test_graph.py         # Tests agent_template memory\n     \u2502   \u251c\u2500\u2500 test_grumpy_agent.py\n     \u2502   \u251c\u2500\u2500 test_requirement_gatherer.py\n+    \u2502   \u251c\u2500\u2500 test_task_manager.py  # Integration tests for Task Manager\n     \u2502   \u2514\u2500\u2500 test_tester_agent.py\n     \u251c\u2500\u2500 testing/                  # Test utilities, evaluators\n     \u2502   \u2514\u2500\u2500 evaluators.py         # LLM-based evaluators (e.g., LLMJudge)"
  }
}