{
  "inputs": {
    "repository": "Moonsong-Labs/ai-nexus",
    "pr_num": "44"
  },
  "outputs": {
    "output": "diff --git a/project_memories/global.md b/project_memories/global.md\nindex 18fd2cc..d10659c 100644\n--- a/project_memories/global.md\n+++ b/project_memories/global.md\n@@ -160,7 +160,7 @@ This file outlines the overarching standards and technological choices for the A\n     *   **Ruff:** Performs code linting and formatting.\n     *   **Mypy:** Conducts static type checking (currently not enforced in CI/default linting pass).\n     *   **codespell:** Checks for spelling mistakes.\n-    *   **openevals:** Suggests involvement in evaluating language models.\n+    *   **openevals:** Used for custom evaluation of agent outputs, for instance, in Coder agent tests where an LLM judges code quality based on a rubric.\n *   **Version Control:** Git.\n *   **LLM Models:**\n     *   **`gemini-1.5-flash-latest` (or similar flash variants):** Preferred for simple tasks, quick evaluations. (PRD mentions `gemini-2.0-flash`, current common models are 1.5 series. The intent is a fast model.)\n@@ -452,7 +452,15 @@ Most agents in AI Nexus follow a common structural and operational pattern, large\n             *   Else: to `END`.\n         *   Edge from `execute_tools` back to `call_model` (to allow the LLM to respond after tool execution).\n *   **`state.py` (`src/coder/state.py`):**\n-    *   `class State(TypedDict): messages: Annotated[list, add_messages]`\n+    ```python\n+    from typing import Annotated, TypedDict\n+    from langchain_core.messages import AnyMessage\n+    from langgraph.graph.message import add_messages\n+\n+    class State(TypedDict):\n+        \"\"\"State for the coder agent.\"\"\"\n+        messages: Annotated[list[AnyMessage], add_messages]\n+    ```\n *   **`README.md` (`src/coder/README.md`):**\n     *   Instructions for setting up a GitHub App with necessary permissions (Contents R/W, Pull requests R/W, Commit statuses R, Issues R/W, Metadata R) and environment variables (`GITHUB_APP_ID`, `GITHUB_APP_PRIVATE_KEY`, `GITHUB_REPOSITORY`).\n \n@@ -642,6 +650,29 @@ The project uses `pytest` for testing and integrates with LangSmith for evaluati\n         *   Tests the grumpy agent against a LangSmith dataset (e.g., `LANGSMITH_DATASET_NAME = \"grumpy-failed-questions\"`).\n         *   Uses `LLMJudge` from `tests.testing.evaluators` to create a `correctness_evaluator` with a specific prompt for judging Grumpy's output.\n         *   The `create_graph_caller` utility is used to wrap the Grumpy agent's graph for evaluation.\n+    *   **`test_coder.py`:**\n+        *   Introduces custom evaluation for the Coder agent.\n+        *   `invoke_agent(inputs: CodeEvaluatorInputs) -> dict`:\n+            *   Sets up `MockGithubApi` with initial files from `inputs[\"starting_code\"]`.\n+            *   Uses `get_github_tools(mock_api)` (from `coder.tools`) to get mocked GitHub tools.\n+            *   Compiles the Coder agent's graph (`graph_builder(github_tools).compile()`).\n+            *   Invokes the graph with `inputs[\"user_input\"]` (as `HumanMessage` within a `coder.state.State` object) and a unique `thread_id`.\n+            *   Returns the full result (final state) from the graph invocation.\n+        *   Custom Evaluation Logic (defined within `test_coder.py`):\n+            *   `EVAL_PROMPT`: A detailed prompt for an LLM to act as an expert code reviewer. It grades the Coder agent's trajectory (branch creation, changes satisfying expectations, no unrelated changes, no bugs) based on `starting_code`, `user_input`, `expectations`, and the agent's `outputs` (trajectory/messages and tool calls).\n+            *   `CodeEvaluatorInputs(TypedDict)`: Defines input structure (`starting_code: dict`, `user_input: str`).\n+            *   `CodeEvaluatorReferenceOutputs(TypedDict)`: Defines reference structure (`expectations: str`).\n+            *   `Result(TypedDict)`: Defines the structured output from the judge LLM (`score: float`, `comment: str`).\n+            *   `judge_llm`: An LLM (e.g., `google_genai:gemini-2.0-flash`) initialized using `init_chat_model` (from `langchain.chat_models`) and configured for structured output (`Result`).\n+            *   `evaluate_code_scorer(inputs, outputs, reference_outputs) -> EvaluatorResult`:\n+                *   Invokes `judge_llm` with the `EVAL_PROMPT` formatted with the provided data.\n+                *   Returns an `EvaluatorResult` from `openevals.types` containing score and comment.\n+            *   `evaluate_code(inputs, outputs, reference_outputs) -> EvaluatorResult`:\n+                *   A wrapper using `openevals.utils._arun_evaluator` to run the `evaluate_code_scorer`.\n+        *   Example Test: `test_coder_creates_rest_api()`\n+            *   Defines `inputs` (user request, initial code state) and `expectations`.\n+            *   Calls `invoke_agent` to get the Coder agent's output (the final state, which includes messages and tool interactions).\n+            *   Calls `evaluate_code` to get the evaluation result based on the agent's trajectory.\n \n *   **`tests/testing/__init__.py`:**\n     *   `get_logger()`: Utility to create a Python logger with a default format.\n@@ -760,6 +791,7 @@ ai-nexus/\n     \u251c\u2500\u2500 datasets/                 # Scripts for creating LangSmith datasets\n     \u2502   \u2514\u2500\u2500 requirement_gatherer_dataset.py\n     \u251c\u2500\u2500 integration_tests/        # Integration tests for agents and full graph functionality\n+    \u2502   \u251c\u2500\u2500 test_coder.py         # Custom evaluation tests for Coder agent\n     \u2502   \u251c\u2500\u2500 test_graph.py         # Tests agent_template memory\n     \u2502   \u251c\u2500\u2500 test_grumpy_agent.py\n     \u2502   \u251c\u2500\u2500 test_requirement_gatherer.py"
  }
}